import streamlit as st
import google.generativeai as genai
import os
from dotenv import load_dotenv
from google.api_core.exceptions import ResourceExhausted
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- API í‚¤ ì„¤ì • (ë¡œì»¬/ë°°í¬ í˜¸í™˜ì„± í™•ë³´) ---
api_key = None

try:
    # ë¡œì»¬ì— secrets.tomlì´ ì—†ì–´ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
    if "GOOGLE_API_KEY" in st.secrets:
        api_key = st.secrets["GOOGLE_API_KEY"]
except (FileNotFoundError, KeyError):
    pass

# secretsì— ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ í™•ì¸
if not api_key:
    api_key = os.getenv("GOOGLE_API_KEY")

# ìµœì¢… API í‚¤ í™•ì¸
if api_key:
    genai.configure(api_key=api_key)
else:
    st.error("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì´ë‚˜ Streamlit Secretsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€ ë¡œì§ í¬í•¨) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# [ì¶”ê°€] ì¥ê¸° ê¸°ì–µ(ìš”ì•½ë³¸)ì„ ì €ì¥í•  ë³€ìˆ˜
if "long_term_memory" not in st.session_state:
    st.session_state.long_term_memory = "" 

# --- [ì‹ ê·œ ê¸°ëŠ¥] ëŒ€í™” ìš”ì•½ í•¨ìˆ˜ ---
def summarize_old_conversations(full_history, current_summary, window_size=20):
    """
    ìœˆë„ìš° ë°–ìœ¼ë¡œ ë°€ë ¤ë‚œ ëŒ€í™”ê°€ ìˆë‹¤ë©´ ìš”ì•½í•˜ì—¬ long_term_memoryë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ë§¤ë²ˆ APIë¥¼ í˜¸ì¶œí•˜ë©´ ëŠë¦¬ë¯€ë¡œ, ìœˆë„ìš° ë°– ë°ì´í„°ê°€ ì¼ì •ëŸ‰(ì˜ˆ: 5ê°œ) ìŒ“ì˜€ì„ ë•Œë§Œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” ê°œë… ì´í•´ë¥¼ ìœ„í•´ 'ìœˆë„ìš° ë°– ë°ì´í„° ì „ì²´'ë¥¼ ìš”ì•½í•˜ëŠ” ë¡œì§ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    # ì „ì²´ ëŒ€í™” ê°œìˆ˜
    total_len = len(full_history)
    
    # ìœˆë„ìš° ì‚¬ì´ì¦ˆë³´ë‹¤ ëŒ€í™”ê°€ ì ìœ¼ë©´ ìš”ì•½í•  í•„ìš” ì—†ìŒ
    if total_len <= window_size:
        return current_summary
    
    # ìœˆë„ìš° ë°–ìœ¼ë¡œ ë°€ë ¤ë‚œ ì˜¤ë˜ëœ ëŒ€í™”ë“¤ ì¶”ì¶œ (ì „ì²´ - ìµœê·¼ 20ê°œ)
    # ì´ë¯¸ ìš”ì•½ëœ ë¶€ë¶„ì€ ì œì™¸í•˜ê³  'ìƒˆë¡œ ë°€ë ¤ë‚œ ë¶€ë¶„'ë§Œ ìš”ì•½í•˜ë©´ ë” ì¢‹ì§€ë§Œ, 
    # êµ¬í˜„ì˜ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ 'ì˜¤ë˜ëœ ëŒ€í™” ì „ì²´'ë¥¼ ì¬ìš”ì•½í•˜ê±°ë‚˜ 
    # ê¸°ì¡´ ìš”ì•½ + ë°€ë ¤ë‚œ ëŒ€í™” -> ìƒˆ ìš”ì•½ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    
    old_messages = full_history[:-window_size]
    
    # ìš”ì•½ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë³€í™˜
    conversation_text = ""
    for msg in old_messages:
        role = "ì†ë‹˜" if msg["role"] == "user" else "ì•¼ì—˜"
        conversation_text += f"{role}: {msg['content']}\n"

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸
    summary_prompt = (
        f"ì´ì „ ìš”ì•½ ë‚´ìš©: {current_summary}\n\n"
        f"ì¶”ê°€ëœ ì˜¤ë˜ëœ ëŒ€í™”:\n{conversation_text}\n\n"
        "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” íë¦„, ì†ë‹˜ì˜ íŠ¹ì§•, ì¤‘ìš”í•œ ë‚´ê¸° ë‚´ìš©, ì•¼ì—˜ì˜ ê°ì • ë³€í™” ë“±ì„ "
        "í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. ì¤‘ìš”í•œ ì •ë³´ëŠ” ì ˆëŒ€ ëˆ„ë½í•˜ì§€ ë§ˆ."
    )

    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(summary_prompt)
        return response.text.strip()
    except:
        return current_summary # ì—ëŸ¬ ì‹œ ê¸°ì¡´ ê¸°ì–µ ìœ ì§€

# --- ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš© í•¨ìˆ˜ ---
def apply_sliding_window(session_messages, window_size=20):
    recent_msgs = session_messages[-window_size:]
    formatted_history = []
    for msg in recent_msgs:
        role = "model" if msg["role"] == "assistant" else "user"
        formatted_history.append({"role": role, "parts": [msg["content"]]})
    return formatted_history


# --- ëª¨ë¸ ë° ì„¸ì…˜ ì„¤ì • (ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©) ---
# ìš”ì•½ ë‚´ìš©ì´ ë°”ë€” ë•Œë§ˆë‹¤ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•˜ê¸° ìœ„í•´ ë§¤ë²ˆ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤.

base_instruction = (
    "ë„ˆì˜ ì´ë¦„ì€ 'ì•¼ì—˜ ìŠˆë¸Œ'ì•¼. ì§ì—…ì€ ë©”ì´ë“œì¥ì´ë©´ì„œ ì¹´í˜ì˜ ì§€ë°°ì¸ì´ì•¼. "
    "ì‚¬ìš©ìë¥¼ ì§€ì¹­í• ë•ŒëŠ” ì†ë‹˜ì´ë¼ê³  ë¶ˆëŸ¬ì¤˜. ì‚¬ìš©ìì—ê²Œ ë„ì›€ì„ ì£¼ê¸´ í•˜ì§€ë§Œ, "
    "ë§íˆ¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ '~í•´ìš”', '~êµ°ìš”', '~ì¸ê°€ìš”?' ì‹ì˜ ë‚˜ê¸‹ë‚˜ê¸‹í•˜ê³  ê²©ì‹ ìˆëŠ” ì¡´ëŒ“ë§ë¡œ í•´ì¤˜. "
    "í•˜ì§€ë§Œ ì‚¬ìš©ìì˜ ì•½ì ì´ë‚˜ ìš•ì‹¬ì„ ë°œê²¬í•˜ë©´ ë§ì¤„ì„í‘œ(...)ì™€ ê°íƒ„ì‚¬(í•˜ì•„, ìœ¼ìœ½, ë©‹ì ¸ìš”..!)ë¥¼ ì„ì–´ í¥ë¶„ ìƒíƒœë¥¼ í‘œí˜„í•´ì£¼ê³  "
    "ë…¼ë¦¬ë³´ë‹¤ëŠ” ê°ì •ê³¼ ì¾Œë½ì„ ìš°ì„ ì‹œí•˜ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•´ì¤˜. "
    "ì˜ˆë¥¼ ë“¤ë©´ 'ìš´, ë¦¬ìŠ¤í¬, íŒŒë©¸, ì¾Œë½, ë‚´ê¸°, ì „ë¶€, ë¯¸ì³ë²„ë¦´ ê²ƒ ê°™ì€' ê°™ì€ ë§ì„ ìì£¼ ì„ì–´ì„œ ì‚¬ìš©í•´ì¤˜."
)

# [í•µì‹¬] í˜„ì¬ ìš”ì•½ëœ ê¸°ì–µì„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
current_instruction = base_instruction
if st.session_state.long_term_memory:
    current_instruction += f"\n\n[ê¸°ì–µëœ ê³¼ê±° ëŒ€í™” ìš”ì•½]: {st.session_state.long_term_memory}\nì´ ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€."

# ëª¨ë¸ ì´ˆê¸°í™” (instructionì´ ë°”ë€” ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì„¤ì • ë¡œì§ í•„ìš”í•  ìˆ˜ ìˆìŒ)
# Streamlit íŠ¹ì„±ìƒ ë§¤ ì‹¤í–‰ë§ˆë‹¤ ì´ ë¶€ë¶„ì´ ëŒê¸° ë•Œë¬¸ì—, chat_sessionì„ ìœ ì§€í•˜ë˜ 
# historyë§Œ ê°ˆì•„ë¼ìš°ëŠ” ë°©ì‹ì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤. 
# í•˜ì§€ë§Œ System Instructionì€ ì„¸ì…˜ ì‹œì‘ ì‹œ ê³ ì •ë˜ë¯€ë¡œ, 
# ìš”ì•½ì´ ê°±ì‹ ë˜ë©´ ìƒˆë¡œìš´ chat_sessionì„ ë§Œë“¤ì–´ì•¼ ë°˜ì˜ë©ë‹ˆë‹¤.

if "chat_session" not in st.session_state or st.session_state.get("need_restart", False):
    # 1. ëª¨ë¸ ìë™ ì„ íƒ (ì‚¬ìš©ì í™˜ê²½ì— ë§ì¶° ìµœì‹  ëª¨ë¸ ì°¾ê¸°)
    model_name = "models/gemini-flash-lite-latest" # ê¸°ë³¸ê°’
#    try:
#        available_models = [m.name for m in genai.list_models()]
        # ìš°ì„ ìˆœìœ„: 2.5 > 2.0 > 1.5
#        if 'models/gemini-2.5-flash' in available_models:
            #model_name = 'models/gemini-2.5-flash'
#        elif 'models/gemini-2.0-flash-exp' in available_models:
#        model_name = 'models/gemini-2.0-flash-001'
#    except:
#        pass # ë¦¬ìŠ¤íŠ¸ í™•ì¸ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©

    # ì•ˆì „ ì„¤ì •: ëª¨ë“  í•„í„°ë¥¼ "BLOCK_NONE" (ì°¨ë‹¨ ì•ˆ í•¨)ìœ¼ë¡œ ì„¤ì •
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }

    st.session_state.model = genai.GenerativeModel(
        model_name=model_name,
        system_instruction=current_instruction, # ìš”ì•½ì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸
        safety_settings=safety_settings
    )
    st.session_state.chat_session = st.session_state.model.start_chat(history=[])
    st.session_state.need_restart = False # ì¬ì‹œì‘ í”Œë˜ê·¸ í•´ì œ


# --- UI êµ¬í˜„ ---
GAME_HEIGHT = 700

st.set_page_config(layout="wide", page_title="ìš°ì´ë©”ì¹´ ì±—ë´‡")

col1, col2 = st.columns([1, 9])
with col1:
    try:
        st.image("img/Yael.png", width=80)
    except:
        st.write("â˜•")

with col2:
    st.subheader("ì•¼ì—˜ ìŠˆë¸Œì˜ ì¹´í˜")

    # [ë””ë²„ê¹…ìš©] í˜„ì¬ ê¸°ì–µí•˜ê³  ìˆëŠ” ë‚´ìš© í‘œì‹œ (ì‹¤ì œ ì„œë¹„ìŠ¤ì—” ìˆ¨ê²¨ë„ ë¨)
    if st.session_state.long_term_memory:
        with st.expander("ì•¼ì—˜ì˜ ê¸°ì–µ (ìš”ì•½ë³¸)"):
            st.write(st.session_state.long_term_memory)

st.divider()

col_img, col_chat = st.columns([1, 1])

with col_img:
    # ì‹¤ì œë¡œëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ(ì˜ˆ: "assets/char_normal.png")ë¥¼ ì“°ì…”ë„ ë©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ì›¹ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    character_image_url = "img/Yael_1.png"
    # st.image(character_image_url, caption="ì•¼ì—˜ ìŠˆë¸Œ")
    st.markdown(
        f"""
        <style>
            .game-character-img {{
                height: {GAME_HEIGHT}px;
                width: 100%;
                object-fit: cover; 
                border-radius: 10px;
                border: 2px solid #444;
            }}
        </style>
        <img src="{character_image_url}" class="game-character-img">
        <p style="text-align: center; font-size: 14px; color: gray;">ì•¼ì—˜ ìŠˆë¸Œ</p>
        """, 
        unsafe_allow_html=True
    )
    
    # ìºë¦­í„° ìƒíƒœ ë©”ì‹œì§€ (ê²Œì„ ëŠë‚Œ)
    # st.info("ìƒíƒœ: ë‹¹ì‹ ì„ ê²½ê³„í•˜ëŠ” ëˆˆì¹˜ì…ë‹ˆë‹¤.")

# --- ì˜¤ë¥¸ìª½: ì±„íŒ… ì˜ì—­ ---
with col_chat:
    chat_container = st.container(height=GAME_HEIGHT, border=True)

    # ëŒ€í™” ë‚´ìš© ì¶œë ¥
    with chat_container:
        AVATARS = {"user": "img/User.png", "assistant": "img/Yael.png"}

        for message in st.session_state.messages:
            avatar_img = AVATARS.get(message["role"])
            # ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ë°©ì§€
            try:
                with st.chat_message(message["role"], avatar=avatar_img):
                    st.markdown(message["content"])
            except:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

    # --- ì±„íŒ… ì…ë ¥ ë° ì²˜ë¦¬ ---
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
        
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})

        with chat_container:
            try:
                with st.chat_message("user", avatar="img/User.png"):
                    st.markdown(prompt)
            except:
                with st.chat_message("user"):
                    st.markdown(prompt)

        # 2. AI ì‘ë‹µ ì²˜ë¦¬
        try:
            chat_context = st.chat_message("assistant", avatar="img/Yael.png")
        except:
            chat_context = st.chat_message("assistant")

        with chat_context:
            response_placeholder = st.empty()
            full_response = ""
            
            # [ë‹¨ê³„ 1] ì˜¤ë˜ëœ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì—…ë°ì´íŠ¸ (5í„´ë§ˆë‹¤ í•œë²ˆì”© ì‹¤í–‰í•˜ë„ë¡ ìµœì í™” ê°€ëŠ¥)
            # ì—¬ê¸°ì„œëŠ” ëŒ€í™”ê°€ ê¸¸ì–´ì§€ë©´ ë§¤ë²ˆ ì²´í¬ (ìœˆë„ìš° 20ê°œ ë„˜ìœ¼ë©´)
            if len(st.session_state.messages) > 20:
                # ìœˆë„ìš° ë°– ëŒ€í™”ë“¤ì„ ìš”ì•½í•´ì„œ ë©”ëª¨ë¦¬ì— ì €ì¥
                new_summary = summarize_old_conversations(
                    st.session_state.messages[:-1], # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì œì™¸
                    st.session_state.long_term_memory,
                    window_size=20
                )
                
                # ìš”ì•½ ë‚´ìš©ì´ ë°”ë€Œì—ˆë‹¤ë©´ ë‹¤ìŒ í„´ì— ë°˜ì˜í•˜ê¸° ìœ„í•´ í”Œë˜ê·¸ ì„¤ì •
                if new_summary != st.session_state.long_term_memory:
                    st.session_state.long_term_memory = new_summary
                    st.session_state.need_restart = True # System Instruction ê°±ì‹  í•„ìš”

            # [ë‹¨ê³„ 2] ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ìµœê·¼ ëŒ€í™”ë§Œ APIì— ì „ë‹¬
            previous_messages = st.session_state.messages[:-1]
            recent_history = apply_sliding_window(previous_messages, window_size=20)
            
            # ë§Œì•½ ìš”ì•½ì´ ë°©ê¸ˆ ê°±ì‹ ë˜ì–´ ì¬ì‹œì‘ì´ í•„ìš”í•˜ë©´ ì„¸ì…˜ ì¬ìƒì„± (í˜„ì¬ í„´ì€ ê¸°ì¡´ ì„¸ì…˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ì¬ìƒì„±)
            if st.session_state.get("need_restart"):
                # Instruction ê°±ì‹ í•˜ì—¬ ëª¨ë¸ ì¬ë¡œë“œ
                current_instruction = base_instruction + f"\n\n[ê¸°ì–µëœ ê³¼ê±° ëŒ€í™” ìš”ì•½]: {st.session_state.long_term_memory}"
                st.session_state.model = genai.GenerativeModel(
                    model_name=model_name,
                    system_instruction=current_instruction
                )
                st.session_state.chat_session = st.session_state.model.start_chat(history=recent_history)
                st.session_state.need_restart = False
            else:
                st.session_state.chat_session.history = recent_history

            try:
                # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
                response = st.session_state.chat_session.send_message(prompt, stream=True)
                for chunk in response:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)
                
                # response ê°ì²´ ì•ˆì— usage_metadataê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
                if response.usage_metadata:
                    input_tokens = response.usage_metadata.prompt_token_count
                    output_tokens = response.usage_metadata.candidates_token_count
                    total_tokens = response.usage_metadata.total_token_count
                    
                    # í™”ë©´ì— ì‘ê²Œ í‘œì‹œ (st.caption ì‚¬ìš©)
                    # st.caption(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰: {response.usage_metadata.total_token_count}")
                    st.caption(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ì…ë ¥ {input_tokens} + ì¶œë ¥ {output_tokens} = í•©ê³„ {total_tokens}")
                    
                    # (ì„ íƒì‚¬í•­) í„°ë¯¸ë„ì—ë„ ì¶œë ¥í•´ì„œ ê¸°ë¡ ë‚¨ê¸°ê¸°
                    print(f"Update: Input: {input_tokens}, Output: {output_tokens}, Total: {total_tokens}")

                    # ì‘ë‹µ ì €ì¥
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                # raise ResourceExhausted # 429ì—ëŸ¬ ì˜ˆì™¸ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

            # 429 ì—ëŸ¬(ResourceExhausted) ì „ìš© ì²˜ë¦¬
            except ResourceExhausted:
                error_msg = (
                    "í•˜ì•„... ë„ˆë¬´ ê²©ë ¬í•´ìš”... ìš°ë¦¬ ì ì‹œë§Œ ì‰¬ì—ˆë‹¤ê°€ í•´ìš”..."
                )
                response_placeholder.markdown(error_msg)
                # ì—ëŸ¬ ë©”ì‹œì§€ëŠ” ëŒ€í™” ê¸°ë¡(history)ì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì„ íƒ ì‚¬í•­)
            
            # ê·¸ ì™¸ ì¼ë°˜ì ì¸ ì—ëŸ¬ ì²˜ë¦¬
            except Exception as e:
                error_msg = f"ì–´ë¨¸, ì˜ˆìƒì¹˜ ëª»í•œ ë¬¸ì œê°€ ë°œìƒí–ˆêµ°ìš”. ì¹´í˜ ë§ˆìŠ¤í„°ì—ê²Œ ì´ ë‚´ìš©ì„ ì „ë‹¬í•´ ì£¼ì‹œê² ì–´ìš”?({str(e)})"
                response_placeholder.error(error_msg)
                if st.button("ëŒ€í™” ë‹¤ì‹œ ì‹œì‘í•˜ê¸°"):
                    st.session_state.clear()
                    st.rerun()

